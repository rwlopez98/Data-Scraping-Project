{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rwlopez98/Data-Scraping-Project/blob/main/Scraper_BeautifulSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import csv\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to get post data from a page\n",
        "def get_posts_from_page(page_url):\n",
        "    response = requests.get(page_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all the posts (adjusted based on HTML structure)\n",
        "    posts = soup.find_all('li', class_='Item ItemComment noPhotoWrap Role_Member pageBox')  # Adjust if necessary\n",
        "    post_data = []\n",
        "\n",
        "    for post in posts:\n",
        "        try:\n",
        "            # Extract user ID (from the profile link)\n",
        "            user_id_tag = post.find('a', class_='Username js-userCard')\n",
        "            if user_id_tag:\n",
        "                user_id = user_id_tag.get_text(strip=True)\n",
        "            else:\n",
        "                user_id = None\n",
        "\n",
        "            # Extract date (using the time tag)\n",
        "            date_tag = post.find('time')\n",
        "            if date_tag:\n",
        "                date = date_tag.get('title')  # 'title' attribute contains the full date\n",
        "            else:\n",
        "                date = None\n",
        "\n",
        "            # Extract comment content\n",
        "            comment_tag = post.find('div', class_='Message userContent')\n",
        "            if comment_tag:\n",
        "                comment = comment_tag.get_text(strip=True)\n",
        "            else:\n",
        "                comment = None\n",
        "\n",
        "            # Append the post data to the list\n",
        "            post_data.append((user_id, date, comment))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting data for a post: {e}\")\n",
        "            continue\n",
        "\n",
        "    return post_data\n",
        "\n",
        "# Function to scrape posts across multiple pages\n",
        "def scrape_edmunds_forum(base_url, start_page, end_page):\n",
        "    all_posts = []\n",
        "\n",
        "    for page_num in range(start_page, end_page - 1, -1):\n",
        "        print(f\"Scraping page {page_num}...\")\n",
        "        page_url = f\"{base_url}/p{page_num}\"  # Construct the page URL\n",
        "        posts = get_posts_from_page(page_url)\n",
        "        all_posts.extend(posts)\n",
        "\n",
        "        if len(all_posts) >= 5000:  # Stop when we reach 5000 posts\n",
        "            break\n",
        "\n",
        "    return all_posts\n",
        "\n",
        "# URL of the forum page (starting from the most recent page, e.g., page 435)\n",
        "base_url = 'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans'\n",
        "\n",
        "# Scrape the posts\n",
        "posts = scrape_edmunds_forum(base_url, start_page=435, end_page=1)\n",
        "\n",
        "# Create a DataFrame with columns: user_id, date, comment\n",
        "df = pd.DataFrame(posts, columns=[\"user_id\", \"date\", \"comment\"])\n",
        "\n",
        "# Rename the index column (level_0) to 'index'\n",
        "df.reset_index(inplace=True)\n",
        "df.rename(columns={'level_0': 'index'}, inplace=True)\n",
        "\n",
        "# Define the file path to save the CSV\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/UT Assignments/Analytics for Unstructured Data/Assignment 1/edmunds_forum_posts_with_user_data.csv'\n",
        "\n",
        "# Export to CSV to the specified location\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "print(f\"Successfully scraped and saved {len(posts)} posts to DataFrame and exported to CSV at {file_path}\")\n",
        "\n",
        "# download the file to your local machine\n",
        "files.download('edmunds_forum_posts_with_user_data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvZyOuneNhmv",
        "outputId": "0c80e021-6f5f-4578-982f-1500645e6219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Scraping page 435...\n",
            "Scraping page 434...\n",
            "Scraping page 433...\n",
            "Scraping page 432...\n",
            "Scraping page 431...\n",
            "Scraping page 430...\n",
            "Scraping page 429...\n",
            "Scraping page 428...\n",
            "Scraping page 427...\n",
            "Scraping page 426...\n",
            "Scraping page 425...\n",
            "Scraping page 424...\n",
            "Scraping page 423...\n",
            "Scraping page 422...\n",
            "Scraping page 421...\n",
            "Scraping page 420...\n",
            "Scraping page 419...\n",
            "Scraping page 418...\n",
            "Scraping page 417...\n",
            "Scraping page 416...\n",
            "Scraping page 415...\n",
            "Scraping page 414...\n",
            "Scraping page 413...\n",
            "Scraping page 412...\n",
            "Scraping page 411...\n",
            "Scraping page 410...\n",
            "Scraping page 409...\n",
            "Scraping page 408...\n",
            "Scraping page 407...\n",
            "Scraping page 406...\n",
            "Scraping page 405...\n",
            "Scraping page 404...\n",
            "Scraping page 403...\n",
            "Scraping page 402...\n",
            "Scraping page 401...\n",
            "Scraping page 400...\n",
            "Scraping page 399...\n",
            "Scraping page 398...\n",
            "Scraping page 397...\n",
            "Scraping page 396...\n",
            "Scraping page 395...\n",
            "Scraping page 394...\n",
            "Scraping page 393...\n",
            "Scraping page 392...\n",
            "Scraping page 391...\n",
            "Scraping page 390...\n",
            "Scraping page 389...\n",
            "Scraping page 388...\n",
            "Scraping page 387...\n",
            "Scraping page 386...\n",
            "Scraping page 385...\n",
            "Scraping page 384...\n",
            "Scraping page 383...\n",
            "Scraping page 382...\n",
            "Scraping page 381...\n",
            "Scraping page 380...\n",
            "Scraping page 379...\n",
            "Scraping page 378...\n",
            "Scraping page 377...\n",
            "Scraping page 376...\n",
            "Scraping page 375...\n",
            "Scraping page 374...\n",
            "Scraping page 373...\n",
            "Scraping page 372...\n",
            "Scraping page 371...\n",
            "Scraping page 370...\n",
            "Scraping page 369...\n",
            "Scraping page 368...\n",
            "Scraping page 367...\n",
            "Scraping page 366...\n",
            "Scraping page 365...\n",
            "Scraping page 364...\n",
            "Scraping page 363...\n",
            "Scraping page 362...\n",
            "Scraping page 361...\n",
            "Scraping page 360...\n",
            "Scraping page 359...\n",
            "Scraping page 358...\n",
            "Scraping page 357...\n",
            "Scraping page 356...\n",
            "Scraping page 355...\n",
            "Scraping page 354...\n",
            "Scraping page 353...\n",
            "Scraping page 352...\n",
            "Scraping page 351...\n",
            "Scraping page 350...\n",
            "Scraping page 349...\n",
            "Scraping page 348...\n",
            "Scraping page 347...\n",
            "Scraping page 346...\n",
            "Scraping page 345...\n",
            "Scraping page 344...\n",
            "Scraping page 343...\n",
            "Scraping page 342...\n",
            "Scraping page 341...\n",
            "Scraping page 340...\n",
            "Scraping page 339...\n",
            "Scraping page 338...\n",
            "Scraping page 337...\n",
            "Scraping page 336...\n",
            "Scraping page 335...\n",
            "Scraping page 334...\n",
            "Scraping page 333...\n",
            "Scraping page 332...\n",
            "Scraping page 331...\n",
            "Scraping page 330...\n",
            "Scraping page 329...\n",
            "Scraping page 328...\n",
            "Scraping page 327...\n",
            "Scraping page 326...\n",
            "Scraping page 325...\n",
            "Scraping page 324...\n",
            "Scraping page 323...\n",
            "Scraping page 322...\n",
            "Scraping page 321...\n",
            "Scraping page 320...\n",
            "Scraping page 319...\n",
            "Scraping page 318...\n",
            "Scraping page 317...\n",
            "Scraping page 316...\n",
            "Scraping page 315...\n",
            "Scraping page 314...\n",
            "Scraping page 313...\n",
            "Scraping page 312...\n",
            "Scraping page 311...\n",
            "Scraping page 310...\n",
            "Scraping page 309...\n",
            "Scraping page 308...\n",
            "Scraping page 307...\n",
            "Scraping page 306...\n",
            "Scraping page 305...\n",
            "Scraping page 304...\n",
            "Scraping page 303...\n",
            "Scraping page 302...\n",
            "Scraping page 301...\n",
            "Scraping page 300...\n",
            "Scraping page 299...\n",
            "Scraping page 298...\n",
            "Scraping page 297...\n",
            "Scraping page 296...\n",
            "Scraping page 295...\n",
            "Scraping page 294...\n",
            "Scraping page 293...\n",
            "Scraping page 292...\n",
            "Scraping page 291...\n",
            "Scraping page 290...\n",
            "Scraping page 289...\n",
            "Scraping page 288...\n",
            "Scraping page 287...\n",
            "Scraping page 286...\n",
            "Scraping page 285...\n",
            "Scraping page 284...\n",
            "Scraping page 283...\n",
            "Scraping page 282...\n",
            "Scraping page 281...\n",
            "Scraping page 280...\n",
            "Scraping page 279...\n",
            "Scraping page 278...\n",
            "Scraping page 277...\n",
            "Scraping page 276...\n",
            "Scraping page 275...\n",
            "Scraping page 274...\n",
            "Scraping page 273...\n",
            "Scraping page 272...\n",
            "Scraping page 271...\n",
            "Scraping page 270...\n",
            "Scraping page 269...\n",
            "Scraping page 268...\n",
            "Scraping page 267...\n",
            "Scraping page 266...\n",
            "Scraping page 265...\n",
            "Scraping page 264...\n",
            "Scraping page 263...\n",
            "Scraping page 262...\n",
            "Scraping page 261...\n",
            "Scraping page 260...\n",
            "Scraping page 259...\n",
            "Scraping page 258...\n",
            "Scraping page 257...\n",
            "Scraping page 256...\n",
            "Scraping page 255...\n",
            "Scraping page 254...\n",
            "Scraping page 253...\n",
            "Scraping page 252...\n",
            "Scraping page 251...\n",
            "Scraping page 250...\n",
            "Scraping page 249...\n",
            "Scraping page 248...\n",
            "Scraping page 247...\n",
            "Scraping page 246...\n",
            "Scraping page 245...\n",
            "Scraping page 244...\n",
            "Scraping page 243...\n",
            "Scraping page 242...\n",
            "Scraping page 241...\n",
            "Scraping page 240...\n",
            "Scraping page 239...\n",
            "Scraping page 238...\n",
            "Scraping page 237...\n",
            "Scraping page 236...\n",
            "Scraping page 235...\n",
            "Scraping page 234...\n",
            "Scraping page 233...\n",
            "Scraping page 232...\n",
            "Scraping page 231...\n",
            "Scraping page 230...\n",
            "Scraping page 229...\n",
            "Scraping page 228...\n",
            "Scraping page 227...\n",
            "Scraping page 226...\n",
            "Scraping page 225...\n",
            "Successfully scraped and saved 5018 posts to DataFrame and exported to CSV at /content/drive/MyDrive/Colab Notebooks/UT Assignments/Analytics for Unstructured Data/Assignment 1/edmunds_forum_posts_with_user_data.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: edmunds_forum_posts_with_user_data.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d228679d784e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# download the file to your local machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'edmunds_forum_posts_with_user_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: edmunds_forum_posts_with_user_data.csv"
          ]
        }
      ]
    }
  ]
}